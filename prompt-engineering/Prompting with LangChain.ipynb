{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65491c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700c9259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deede6c1-e4e0-4143-8b3c-e31b47522163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea1ffef-824d-4095-83ab-c938083beb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"\" I don't know.\n",
    "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers. \n",
    ".....\n",
    "Question: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template = template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62553b2-8a09-4428-8050-8339ab2f4f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the main advantage of quantum computing over classical computing?\n",
      "Answer: The main advantage of quantum computing over classical computing is the ability to solve complex problems faster.\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt = prompt_template)\n",
    "\n",
    "\n",
    "input_data = {\"query\": \"\"\"What is the main advantage of quantum computing over classical computing?\"\"\"}\n",
    "\n",
    "\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(\"Question:\", input_data['query'])\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcaf061-d49a-4bb9-b927-13489b1afe5b",
   "metadata": {},
   "source": [
    "Few Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11245a-6fb1-4d64-a7b5-ac0df41afcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\"animal\": \"lion\", \"habitat\":\"savanna\"},\n",
    "    { \"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
    "    { \"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
    "]\n",
    "\n",
    "\n",
    "example_template = \"\"\"\n",
    "Animal: {animal}\n",
    "Habitat: {habitat}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"animal\", \"habitat\"],\n",
    "    template=example_template,\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Identify the habitat of the given animal\",\n",
    "    suffix=\"Animal: {input}\\n Habitat: \",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "\n",
    "input_data = {\"input\": \"clams\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8adf573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "loaded_prompt = load_prompt(\"awesome_project.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a0a0e",
   "metadata": {},
   "source": [
    "**Sarcasm using Few Shot Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237a08eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326e19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42171f28-fe2c-44eb-a745-f844db2e69b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Watch every episode of The Big Bang Theory. Step 2: Realize that's not actually how quantum computing works. Step 3: Enroll in a course or read some books on the subject. Good luck!\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do I become a better programmer?\",\n",
    "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Why is the sky blue? \",\n",
    "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate( \n",
    "    input_variables= [\"query\", \"answer\"],\n",
    "template=example_template)\n",
    "\n",
    "\n",
    "\n",
    "prefix= \"\"\"The following are excerpts from conversations with an AI assistant. The assistant is typically sarcasticc and witty, producing creative and funny responses to users' questions. Here are some examples:\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "examples=examples,\n",
    "example_prompt=example_prompt,\n",
    "prefix = prefix,\n",
    "suffix=suffix,\n",
    "input_variables=[\"query\"],\n",
    "example_separator=\"\\n\\n\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "\n",
    "input_data = {\"query\":\"How can I learn quantum computing?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dedbe98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do you feel today?\",\n",
    "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
    "    }, {\n",
    "        \"query\": \"What is the speed of light?\",\n",
    "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
    "    }, {\n",
    "        \"query\": \"What is a quantum computer?\",\n",
    "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
    "    }, {\n",
    "        \"query\": \"Who invented the telephone?\",\n",
    "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
    "    }, {\n",
    "        \"query\": \"What programming language is best for AI development?\",\n",
    "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is photosynthesis?\",\n",
    "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
    "    }, {\n",
    "        \"query\": \"What is the tallest mountain on Earth?\",\n",
    "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the most abundant element in the universe?\",\n",
    "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the largest mammal on Earth?\",\n",
    "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the fastest land animal?\",\n",
    "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the square root of 144?\",\n",
    "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the average temperature on Mars?\",\n",
    "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3fb94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "examples=examples,\n",
    "example_prompt = example_prompt,\n",
    "max_length =100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8a1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "example_selector=example_selector,\n",
    "example_prompt=example_prompt,\n",
    "prefix = prefix,\n",
    "suffix = suffix,\n",
    "input_variables=[\"query\"],\n",
    "example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "562a700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure, but whoever it was must have been really good at confusing people!\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
    "\n",
    "\n",
    "input_data = {\"query\": \"Who invented the perplexity?\"}\n",
    "response = chain.run(input_data)\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4d251",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompts and Example Selectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccd6110a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I be lovin' programmin', matey.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4f90f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='You are a helpful assistant that translates english to pirate.', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Hi', template_format='f-string', validate_template=True), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Argh me mateys', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], output_parser=None, partial_variables={}, template='{text}', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14dd1e9",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dab6654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and doggs, better bring an umbrella?\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "user: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "input_variables = [\"query\" , \"answer\"],\n",
    "template=example_template \n",
    ")\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI assistant. The assistant is known for its humor and wit, providing entertaining and amusing responses to users' questions. Here are some examples:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7731125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The secret to coding is like a good joke - timing and syntax are everything!'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the secret to coding?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4244e",
   "metadata": {},
   "source": [
    "#### Example Selectors\n",
    "Using LengthBased Examples selector to ensure the prompt length doesn't Exceed the specified value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "672afd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4391918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "input_variables=[\"word\", \"antonym\"],\n",
    "template=example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e3a96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "examples = examples,\n",
    "example_prompt=example_prompt,\n",
    "max_length=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a2e2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "example_selector=example_selector,\n",
    "example_prompt=example_prompt,\n",
    "prefix=\"Give the antonym of every input\",\n",
    "suffix=\"Word: {input}\\nAntonym:\",\n",
    "input_variables=[\"input\"],\n",
    "example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "176618f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "\n",
      "\n",
      "Word: energetic\n",
      "Antonym: lethargic\n",
      "\n",
      "\n",
      "\n",
      "Word: sunny\n",
      "Antonym: gloomy\n",
      "\n",
      "\n",
      "Word: tall\n",
      "Antonym:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt.format(input=\"tall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ddb871e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sink'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
    "chain.run(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17387154",
   "metadata": {},
   "source": [
    "**Semantic Similarity ExampleSelector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f076e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d3ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 5 embeddings in 1 batches of size 5:: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (5, 1)      str     None   \n",
      " metadata     json      (5, 1)      str     None   \n",
      " embedding  embedding  (5, 1536)  float32   None   \n",
      "    id        text      (5, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Create a PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "input_variables=[\"input\", \"output\"],\n",
    "template= \"Input: {input}\\nOutput: {output}\",)\n",
    "\n",
    "examples = [\n",
    "    {\"input\":\"0C\", \"output\":\"32F\"},\n",
    "    {\"input\":\"10C\", \"output\":\"50F\"},\n",
    "    {\"input\":\"20C\", \"output\":\"68F\"},\n",
    "    {\"input\":\"30C\", \"output\":\"86F\"},\n",
    "    {\"input\":\"40C\", \"output\":\"104F\"}\n",
    "]\n",
    "\n",
    "my_activeloop_ord_id = \"Ali\"\n",
    "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
    "dataset_path = f\"hub://{my_activeloop_ord_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path)\n",
    "\n",
    "\n",
    "# Embedding function\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples( examples, embeddings, db, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6246953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "example_selector=example_selector,\n",
    "example_prompt=example_prompt,\n",
    "prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
    "suffix=\"Input: {temperature}\\nOutput:\",\n",
    "input_variables=[\"temperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0963ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 10C\n",
      "Output: 50F\n",
      "\n",
      "Input: 10C\n",
      "Output:\n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 30C\n",
      "Output: 86F\n",
      "\n",
      "Input: 30C\n",
      "Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 1 embeddings in 1 batches of size 1:: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (9, 1)      str     None   \n",
      " metadata     json      (9, 1)      str     None   \n",
      " embedding  embedding  (9, 1536)  float32   None   \n",
      "    id        text      (9, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 40C\n",
      "Output: 104F\n",
      "\n",
      "Input: 40C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(similar_prompt.format(temperature=\"10C\"))# Test with an input\n",
    "print(similar_prompt.format(temperature=\"30C\")) # Test with another input\n",
    "\n",
    "similar_prompt.example_selector.add_example({\"input\":\"50C\", \"output\": \"122F\"})\n",
    "print(similar_prompt.format(temperature=\"40C\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25b26170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'158F'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=similar_prompt)\n",
    "chain.run(\"70C\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
