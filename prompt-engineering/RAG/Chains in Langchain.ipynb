{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0d909c",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c839d127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c1d7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirmustafaali/anaconda3/envs/activeloop/lib/python3.11/site-packages/langchain/llms/openai.py:179: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/mirmustafaali/anaconda3/envs/activeloop/lib/python3.11/site-packages/langchain/llms/openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "prompt_template = \"what is a word to replace the follwoing: {word}?\"\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "llm=llm,\n",
    "prompt=PromptTemplate.from_template(prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e093ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': 'synthetic'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8355e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'artificial', 'text': 'synthetic'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"word\":\"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\":\"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain(input_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be699888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='synthetic', generation_info=None)]], llm_output={'token_usage': <OpenAIObject at 0x140a4f7d0> JSON: {\n",
       "  \"prompt_tokens\": 20,\n",
       "  \"completion_tokens\": 2,\n",
       "  \"total_tokens\": 22\n",
       "}, 'model_name': 'gpt-3.5-turbo'}, run=RunInfo(run_id=UUID('ba123767-54ed-4736-a0ff-092f643eebad')))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.generate([input_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "974d6ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'air conditioner'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Looking at the context of '{context}'. \\\n",
    "What is an appropriate word to replace the following: {word}\"\"\"\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "llm_chain.predict(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5214250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supporter'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain.from_string(llm=llm, template=prompt_template)\n",
    "llm_chain.predict(word=\"fan\", context=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ce0ad",
   "metadata": {},
   "source": [
    "### Conversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18adc270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'synthetic, simulated, man-made, manufactured, fake, imitation'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "conversation = ConversationChain(\n",
    "llm = llm,\n",
    "memory = ConversationBufferMemory())\n",
    "\n",
    "\n",
    "conversation.predict(input=\"\"\"List all possible words as substitute for 'artifical' as comma separated.\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7c54002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'replica, imitation, faux, pseudo'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9021e",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "```\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chain=[chain_one, chain_two])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6401f05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02b3166",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce6b7c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'synthetic, man-made, fake, imitation, simulated, faux, counterfeit, ersatz, fabricated, manufactured'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "llm=llm,\n",
    "    prompt=PromptTemplate(template=template,\n",
    "                         input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory = ConversationBufferMemory(),\n",
    "    verbose=True )\n",
    "conversation.predict(input=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18e7d3",
   "metadata": {},
   "source": [
    "## Custom Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b484ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenatedChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "    \n",
    "    \n",
    "    def _call(self, inputs:Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb144257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated output: \n",
      " Artificial means made or produced by human beings rather than occurring naturally. It can also refer to something that is not genuine or real.Synthetic\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "input_variables = [\"word\"],\n",
    "template = \"What is the meaning of the following word '{word}'?\")\n",
    "\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2= PromptTemplate(\n",
    "input_variables=[\"word\"],\n",
    "template = \"What is a word to replace the following: {word}?\")\n",
    "\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "\n",
    "concat_chain = ConcatenatedChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"concatenated output: \\n {concat_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe95084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
